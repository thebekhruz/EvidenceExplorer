{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was used to examine how different values affect the accuracy of the prediction.\n",
    "```bash\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def grid_search_doc2vec_parameters(tagged_data, df_train, df_test, y_test, param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    all_results = []\n",
    "\n",
    "    for vector_size, window, min_count, epochs in itertools.product(*param_grid.values()):\n",
    "        print(f\"Training Doc2Vec with vector_size={vector_size}, window={window}, min_count={min_count}, epochs={epochs}...\")\n",
    "        \n",
    "        # Train Doc2Vec model\n",
    "        model = train_doc2vec_model(tagged_data, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, workers=4)\n",
    "        \n",
    "        # Infer vectors for training and testing datasets\n",
    "        vectors_train = infer_vectors(model, tagged_data)\n",
    "        X_train = vectors_train\n",
    "        y_train = df_train['label']\n",
    "        \n",
    "        vectors_test = infer_vectors(model, create_tagged_document(df_test))\n",
    "        X_test = vectors_test\n",
    "        \n",
    "        # Train Logistic Regression model\n",
    "        clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        # Keep track of the best parameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'vector_size': vector_size, 'window': window, 'min_count': min_count, 'epochs': epochs}\n",
    "        \n",
    "        # Store all results\n",
    "        all_results.append((accuracy, vector_size, window, min_count, epochs))\n",
    "\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    return best_params, all_results\n",
    "\n",
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "    'vector_size': [768, 400, 500, 300],  # Example sizes\n",
    "    'window': [2],           # Example window sizes\n",
    "    'min_count': [2],        # Example min_count values\n",
    "    'epochs': [20]          # Example epoch counts\n",
    "}\n",
    "\n",
    "# Run the grid search\n",
    "best_params, all_results = grid_search_doc2vec_parameters(tagged_data_train, df_train, df_test, df_test['label'], param_grid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the whole notebook to see the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For text preprocessing\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# For text vectorization we will use Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# For the classifier we will use Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For evaluation we will use accuracy, f1-score, precesion, recall and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should legalize the growing of coca leaf</td>\n",
       "      <td>Robert W. Sweet, a federal judge, strongly agr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should ban trans fats usage in food</td>\n",
       "      <td>The net increase in LDL/HDL ratio with trans f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should legalize prostitution</td>\n",
       "      <td>Pertaining to health, safety and services, the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We should subsidize investigative journalism</td>\n",
       "      <td>Date granted: 10 June 2002 Citation: \"For serv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We should abolish homework</td>\n",
       "      <td>The Yarrabah community has a public library wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Claim  \\\n",
       "0   We should legalize the growing of coca leaf   \n",
       "1        We should ban trans fats usage in food   \n",
       "2               We should legalize prostitution   \n",
       "3  We should subsidize investigative journalism   \n",
       "4                    We should abolish homework   \n",
       "\n",
       "                                            Evidence  label  \n",
       "0  Robert W. Sweet, a federal judge, strongly agr...      1  \n",
       "1  The net increase in LDL/HDL ratio with trans f...      1  \n",
       "2  Pertaining to health, safety and services, the...      0  \n",
       "3  Date granted: 10 June 2002 Citation: \"For serv...      0  \n",
       "4  The Yarrabah community has a public library wh...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets for training and testing\n",
    "filepath_train = '/Users/thebekhruz/Desktop/nlu/EvidenceExplorer/data/train/train.csv'\n",
    "filepath_test = '/Users/thebekhruz/Desktop/nlu/EvidenceExplorer/data/validate/dev.csv'\n",
    "\n",
    "df_train = pd.read_csv(filepath_train)\n",
    "df_test = pd.read_csv(filepath_test)\n",
    "\n",
    "# Check the first 5 rows of the training dataset\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create tagged documents\n",
    "# This is required for Doc2Vec to train the model\n",
    "\n",
    "# We will use spaCy to tokenize and preprocess the text and then create tagged documents\n",
    "# Each document is tagged with the index of the row in the dataframe\n",
    "\n",
    "\n",
    "def create_tagged_document(df):\n",
    "    tagged_data = []\n",
    "    for i, text in enumerate(df['Claim'] + ' ' + df['Evidence']):\n",
    "        # Process the text with the spaCy language model\n",
    "        doc = nlp(text)\n",
    "        # Tokenize and lemmatize the text, removing stopwords\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        # Create a TaggedDocument for each row in the dataframe\n",
    "        tagged_data.append(TaggedDocument(words=tokens, tags=[str(i)]))  # Tags are typically strings\n",
    "    return tagged_data\n",
    "\n",
    "\n",
    "tagged_data_train = create_tagged_document(df_train)\n",
    "tagged_data_test = create_tagged_document(df_test)\n",
    "\n",
    "\n",
    "# Save the tagged documents to disk\n",
    "import pickle\n",
    "\n",
    "with open('tagged_data_train.pkl', 'wb') as f:\n",
    "    pickle.dump(tagged_data_train, f)\n",
    "\n",
    "with open('tagged_data_test.pkl', 'wb') as f:\n",
    "    pickle.dump(tagged_data_test, f)\n",
    "\n",
    "\n",
    "\n",
    "# Takes around 6 min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Doc2Vec model\n",
    "# We will use a simple model with a vector size of 100 and a window size of 2\n",
    "# We will train the model for 20 epochs\n",
    "\n",
    "def train_doc2vec_model(tagged_data, vector_size=768, window=2, min_count=1, epochs=20, workers=4):\n",
    "    model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "# Call the funciton to train the model\n",
    "# model = train_doc2vec_model(tagged_data_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Emeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the vectors for the training data\n",
    "def infer_vectors(model, tagged_documents):\n",
    "    vectors = [model.infer_vector(doc.words) for doc in tagged_documents]\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Infer the vectors for the training data\n",
    "# vectors_train = infer_vectors(model, tagged_data_train)\n",
    "# vectors_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exctract features and labels\n",
    "# X = vectors_train\n",
    "# y = df_train['label']\n",
    "\n",
    "\n",
    "# # Train Logistic Regression model\n",
    "# clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "# clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     53\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m512\u001b[39m],  \u001b[38;5;66;03m# Example sizes\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m6\u001b[39m],         \u001b[38;5;66;03m# Example window sizes\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_count\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m6\u001b[39m],      \u001b[38;5;66;03m# Example min_count values\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m20\u001b[39m]         \u001b[38;5;66;03m# Example epoch counts\u001b[39;00m\n\u001b[1;32m     58\u001b[0m }\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Run the grid search\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m best_params, all_results \u001b[38;5;241m=\u001b[39m grid_search_doc2vec_parameters(tagged_data_train, \u001b[43mtagged_data_test\u001b[49m, y_train, y_test, param_grid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def grid_search_doc2vec_parameters(tagged_data_train, tagged_data_test, y_train, y_test, param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    all_results = []\n",
    "\n",
    "    # Precompute the test vectors since the test set doesn't change during parameter tuning\n",
    "    # Initialize a model for infer_vector method availability; parameters here are not critical\n",
    "    model_for_infer = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=20)\n",
    "    model_for_infer.build_vocab(tagged_data_train)  # Just to prepare the model for inference\n",
    "    vectors_test = np.array([model_for_infer.infer_vector(doc.words) for doc in tagged_data_test])\n",
    "\n",
    "    for vector_size, window, min_count, epochs in itertools.product(*param_grid.values()):\n",
    "        print(f\"Training Doc2Vec with vector_size={vector_size}, window={window}, min_count={min_count}, epochs={epochs}...\")\n",
    "        \n",
    "        # Train Doc2Vec model\n",
    "        model = train_doc2vec_model(tagged_data_train, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs, workers=4)\n",
    "        \n",
    "        # Infer vectors for training datasets\n",
    "        vectors_train = np.array([model.infer_vector(doc.words) for doc in tagged_data_train])\n",
    "        \n",
    "        # Train Logistic Regression model\n",
    "        clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "        clf.fit(vectors_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = clf.predict(vectors_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        # Keep track of the best parameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'vector_size': vector_size, 'window': window, 'min_count': min_count, 'epochs': epochs}\n",
    "        \n",
    "        # Store all results\n",
    "        all_results.append((accuracy, vector_size, window, min_count, epochs))\n",
    "\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    return best_params, all_results\n",
    "\n",
    "# Assuming tagged_data_train and tagged_data_test are already created\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Define your parameter grid\n",
    "param_grid = {\n",
    "    'vector_size': [512],  # Example sizes\n",
    "    'window': [2,4,6],         # Example window sizes\n",
    "    'min_count': [2,4,6],      # Example min_count values\n",
    "    'epochs': [20]         # Example epoch counts\n",
    "}\n",
    "\n",
    "# Run the grid search\n",
    "best_params, all_results = grid_search_doc2vec_parameters(tagged_data_train, tagged_data_test, y_train, y_test, param_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
